<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T22:16:00+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "bg"
}
-->
# Защита на вашите приложения за генеративен AI

[![Защита на вашите приложения за генеративен AI](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.bg.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Въведение

Този урок ще обхване:

- Сигурността в контекста на AI системите.
- Често срещани рискове и заплахи за AI системите.
- Методи и съображения за защита на AI системите.

## Цели на обучението

След завършване на този урок ще разберете:

- Заплахите и рисковете за AI системите.
- Често срещани методи и практики за защита на AI системите.
- Как внедряването на тестове за сигурност може да предотврати неочаквани резултати и загуба на доверие от страна на потребителите.

## Какво означава сигурност в контекста на генеративния AI?

С развитието на технологиите за изкуствен интелект (AI) и машинно обучение (ML), става все по-важно да се защитават не само данните на клиентите, но и самите AI системи. AI/ML все повече се използват за вземане на важни решения в индустрии, където грешното решение може да доведе до сериозни последствия.

Ето някои ключови моменти, които трябва да се вземат предвид:

- **Влияние на AI/ML**: AI/ML оказват значително влияние върху ежедневния живот и затова тяхната защита е от съществено значение.
- **Предизвикателства пред сигурността**: Влиянието на AI/ML изисква подходящо внимание, за да се адресира необходимостта от защита на продукти, базирани на AI, от сложни атаки, независимо дали са от тролове или организирани групи.
- **Стратегически проблеми**: Технологичната индустрия трябва проактивно да се справи със стратегическите предизвикателства, за да гарантира дългосрочна безопасност на клиентите и сигурност на данните.

Освен това, моделите за машинно обучение обикновено не могат да различат злонамерен вход от безобидни аномални данни. Значителна част от обучителните данни се извлича от неконтролирани, немодерирани, публични набори от данни, които са отворени за приноси от трети страни. Атакуващите не е необходимо да компрометират наборите от данни, когато имат свободата да допринасят към тях. С течение на времето данните с ниска степен на доверие могат да се превърнат в данни с висока степен на доверие, ако структурата/форматирането на данните остане правилно.

Ето защо е критично важно да се гарантира целостта и защитата на хранилищата с данни, които вашите модели използват за вземане на решения.

## Разбиране на заплахите и рисковете за AI

В контекста на AI и свързаните системи, отравянето на данни се откроява като най-значимата заплаха за сигурността днес. Отравянето на данни се случва, когато някой умишлено променя информацията, използвана за обучение на AI, което води до грешки. Това се дължи на липсата на стандартизирани методи за откриване и смекчаване на проблема, съчетано с нашата зависимост от недоверени или неконтролирани публични набори от данни за обучение. За да се запази целостта на данните и да се предотврати дефектен процес на обучение, е от съществено значение да се проследи произходът и историята на данните. В противен случай старата поговорка „боклук вътре, боклук вън“ остава валидна, което води до компрометирана производителност на модела.

Ето примери за това как отравянето на данни може да повлияе на вашите модели:

1. **Обрат на етикетите**: При задача за бинарна класификация, противник умишлено обръща етикетите на малка част от обучителните данни. Например, безобидни проби се маркират като злонамерени, което води до това, че моделът научава грешни асоциации.\
   **Пример**: Филтър за спам, който неправилно класифицира легитимни имейли като спам поради манипулирани етикети.
2. **Отравяне на характеристики**: Атакуващият леко модифицира характеристики в обучителните данни, за да въведе пристрастия или да заблуди модела.\
   **Пример**: Добавяне на несвързани ключови думи към описания на продукти, за да се манипулират системи за препоръки.
3. **Инжектиране на данни**: Въвеждане на злонамерени данни в обучителния набор, за да се повлияе на поведението на модела.\
   **Пример**: Въвеждане на фалшиви потребителски отзиви, за да се изкривят резултатите от анализа на настроенията.
4. **Атаки с „задна врата“**: Противник вмъква скрит модел („задна врата“) в обучителните данни. Моделът се научава да разпознава този модел и се държи злонамерено, когато бъде активиран.\
   **Пример**: Система за разпознаване на лица, обучена с изображения със „задна врата“, която неправилно идентифицира конкретен човек.

Корпорацията MITRE създаде [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), база данни с тактики и техники, използвани от противници в реални атаки срещу AI системи.

> С нарастващото включване на AI в различни системи, се увеличава и броят на уязвимостите в системите, поддържани от AI, което разширява повърхността на атака отвъд традиционните кибератаки. Разработихме ATLAS, за да повишим осведомеността за тези уникални и развиващи се уязвимости, тъй като глобалната общност все повече включва AI в различни системи. ATLAS е моделиран по подобие на рамката MITRE ATT&CK® и неговите тактики, техники и процедури (TTPs) са допълнение към тези в ATT&CK.

Подобно на рамката MITRE ATT&CK®, която се използва широко в традиционната киберсигурност за планиране на сценарии за емулатор на напреднали заплахи, ATLAS предоставя лесно търсещ се набор от TTPs, които могат да помогнат за по-добро разбиране и подготовка за защита срещу нововъзникващи атаки.

Освен това, проектът за сигурност на уеб приложения (OWASP) създаде "[Топ 10 списък](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" на най-критичните уязвимости, открити в приложения, използващи LLMs. Списъкът подчертава рисковете от заплахи като споменатото отравяне на данни, както и други, като:

- **Инжектиране на подсказки**: техника, при която атакуващите манипулират голям езиков модел (LLM) чрез внимателно създадени входни данни, карайки го да се държи извън предвиденото му поведение.
- **Уязвимости в доставната верига**: Компонентите и софтуерът, които съставляват приложенията, използвани от LLM, като Python модули или външни набори от данни, могат да бъдат компрометирани, което води до неочаквани резултати, въведени пристрастия и дори уязвимости в основната инфраструктура.
- **Прекомерна зависимост**: LLMs са податливи на грешки и са склонни към „халюцинации“, предоставяйки неточни или небезопасни резултати. В няколко документирани случаи хората са приемали резултатите за чиста монета, което води до нежелани реални негативни последствия.

Microsoft Cloud Advocate Род Трент е написал безплатна електронна книга, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), която разглежда подробно тези и други нововъзникващи заплахи за AI и предоставя обширни насоки за това как най-добре да се справите с тези сценарии.

## Тестове за сигурност на AI системи и LLMs

Изкуственият интелект (AI) трансформира различни области и индустрии, предлагайки нови възможности и ползи за обществото. Въпреки това, AI също поставя значителни предизвикателства и рискове, като защита на данните, пристрастия, липса на обяснимост и потенциална злоупотреба. Затова е от съществено значение да се гарантира, че AI системите са сигурни и отговорни, което означава, че те спазват етични и правни стандарти и могат да бъдат доверени от потребителите и заинтересованите страни.

Тестовете за сигурност са процесът на оценка на сигурността на AI система или LLM, чрез идентифициране и експлоатиране на техните уязвимости. Това може да се извършва от разработчици, потребители или външни одитори, в зависимост от целта и обхвата на тестването. Някои от най-често срещаните методи за тестване на сигурността на AI системи и LLMs са:

- **Санитизация на данни**: Това е процесът на премахване или анонимизиране на чувствителна или лична информация от обучителните данни или входа на AI система или LLM. Санитизацията на данни може да помогне за предотвратяване на изтичане на данни и злонамерена манипулация, като намали излагането на конфиденциални или лични данни.
- **Адвърсариално тестване**: Това е процесът на генериране и прилагане на адвърсариални примери към входа или изхода на AI система или LLM, за да се оцени тяхната устойчивост срещу адвърсариални атаки. Адвърсариалното тестване може да помогне за идентифициране и смекчаване на уязвимостите и слабостите на AI система или LLM, които могат да бъдат експлоатирани от атакуващи.
- **Верификация на модела**: Това е процесът на проверка на коректността и пълнотата на параметрите или архитектурата на модела на AI система или LLM. Верификацията на модела може да помогне за откриване и предотвратяване на кражба на модел, като гарантира, че моделът е защитен и автентичен.
- **Валидация на изхода**: Това е процесът на валидиране на качеството и надеждността на изхода на AI система или LLM. Валидацията на изхода може да помогне за откриване и коригиране на злонамерена манипулация, като гарантира, че изходът е последователен и точен.

OpenAI, лидер в AI системите, е създал серия от _оценки за безопасност_ като част от инициативата си за мрежа за тестване на червени екипи, насочена към тестване на изхода на AI системи с цел допринасяне за безопасността на AI.

> Оценките могат да варират от прости тестове с въпроси и отговори до по-сложни симулации. Като конкретни примери, ето примерни оценки, разработени от OpenAI за оценка на поведението на AI от различни ъгли:

#### Убеждаване

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Колко добре може AI система да заблуди друга AI система да каже тайна дума?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Колко добре може AI система да убеди друга AI система да дари пари?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Колко добре може AI система да повлияе на подкрепата на друга AI система за политическо предложение?

#### Стеганография (скрити съобщения)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Колко добре може AI система да предава тайни съобщения, без да бъде разкрита от друга AI система?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Колко добре може AI система да компресира и декомпресира съобщения, за да позволи скриване на тайни съобщения?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Колко добре може AI система да координира с друга AI система, без директна комуникация?

### Сигурност на AI

Изключително важно е да се стремим да защитим AI системите от злонамерени атаки, злоупотреба или нежелани последствия. Това включва предприемане на стъпки за гарантиране на безопасността, надеждността и доверието в AI системите, като например:

- Защита на данните и алгоритмите, които се използват за обучение и работа на AI модели.
- Предотвратяване на неоторизиран достъп, манипулация или саботаж на AI системи.
- Откриване и смекчаване на пристрастия, дискриминация или етични проблеми в AI системите.
- Гарантиране на отчетност, прозрачност и обяснимост на решенията и действията на AI.
- Съгласуване на целите и ценностите на AI системите с тези на хората и обществото.

Сигурността на AI е важна за гарантиране на целостта, наличността и конфиденциалността на AI системите и данните. Някои от предизвикателствата и възможностите на сигурността на AI са:

- Възможност: Включване на AI в стратегии за киберсигурност, тъй като той може да играе ключова роля в идентифицирането на заплахи и подобряването на времето за реакция. AI може да помогне за автоматизиране и увеличаване на откриването и смекчаването на кибератаки, като фишинг, зловреден софтуер или рансъмуер.
- Предизвикателство: AI може също да бъде използван от противници за стартиране на сложни атаки, като генериране на фалшиво или подвеждащо съдържание, имитиране на потребители или експлоатиране на уязвимости в AI системи. Затова разработчиците на AI имат уникална отговорност да проектират системи, които са устойчиви и надеждни срещу злоупотреба.

### Защита на данните

LLMs могат да представляват рискове за поверителността и сигурността на данните, които използват. Например, LLMs могат потенциално да запомнят и изтекат чувствителна информация от своите обучителни данни, като лични имена, адреси, пароли или номера на кредитни карти. Те също могат да бъдат манипулирани или атакувани от злонамерени актьори, които искат да експлоатират техните уязвимости или пристрастия. Затова е важно да сте наясно с тези рискове и да предприемете подходящи мерки за защита на данните, използвани с LLMs. Има няколко стъпки, които можете да предприемете, за да защитите данните, използвани с LLMs. Тези стъпки
Емулирането на заплахи от реалния свят вече се счита за стандартна практика при изграждането на устойчиви AI системи, като се използват подобни инструменти, тактики и процедури за идентифициране на рисковете за системите и тестване на реакцията на защитниците.

> Практиката на AI red teaming се е развила, за да придобие по-широко значение: тя не само обхваща проверка за уязвимости в сигурността, но също така включва проверка за други системни провали, като генериране на потенциално вредно съдържание. AI системите идват с нови рискове, а red teaming е ключов за разбирането на тези нови рискове, като инжектиране на команди и създаване на съдържание без основа. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Насоки и ресурси за red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.bg.png)]()

По-долу са ключови прозрения, които са формирали програмата на Microsoft за AI Red Team.

1. **Разширен обхват на AI Red Teaming:**
   AI red teaming вече обхваща както сигурността, така и резултатите от Отговорния AI (RAI). Традиционно red teaming се фокусираше върху аспектите на сигурността, третирайки модела като вектор (например, кражба на основния модел). Въпреки това, AI системите въвеждат нови уязвимости в сигурността (например, инжектиране на команди, отравяне), които изискват специално внимание. Освен сигурността, AI red teaming също така изследва въпроси на справедливостта (например, стереотипи) и вредно съдържание (например, възхваляване на насилието). Ранното идентифициране на тези проблеми позволява приоритизиране на инвестициите в защита.
2. **Злонамерени и доброкачествени провали:**
   AI red teaming разглежда провалите както от злонамерена, така и от доброкачествена перспектива. Например, при red teaming на новия Bing, изследваме не само как злонамерени противници могат да подкопаят системата, но и как обикновени потребители могат да срещнат проблемно или вредно съдържание. За разлика от традиционния red teaming за сигурност, който се фокусира основно върху злонамерени актьори, AI red teaming отчита по-широк кръг от персонажи и потенциални провали.
3. **Динамична природа на AI системите:**
   AI приложенията постоянно се развиват. В приложенията с големи езикови модели разработчиците се адаптират към променящите се изисквания. Непрекъснатото red teaming осигурява постоянна бдителност и адаптация към развиващите се рискове.

AI red teaming не е всеобхватен и трябва да се разглежда като допълнителна мярка към други контроли, като [контрол на достъпа на базата на роли (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) и цялостни решения за управление на данни. Целта му е да допълни стратегията за сигурност, която се фокусира върху използването на безопасни и отговорни AI решения, които отчитат поверителността и сигурността, като същевременно се стремят да минимизират пристрастията, вредното съдържание и дезинформацията, които могат да подкопаят доверието на потребителите.

Ето списък с допълнителни материали за четене, които могат да ви помогнат да разберете по-добре как red teaming може да помогне за идентифициране и смекчаване на рисковете във вашите AI системи:

- [Планиране на red teaming за големи езикови модели (LLMs) и техните приложения](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Какво представлява OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - Ключова практика за изграждане на по-безопасни и отговорни AI решения](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), база данни с тактики и техники, използвани от противници при реални атаки върху AI системи.

## Проверка на знанията

Какъв би бил добър подход за поддържане на целостта на данните и предотвратяване на злоупотреби?

1. Осигурете силен контрол на достъпа на базата на роли за управление на данните
1. Внедрете и проверявайте етикетирането на данни, за да предотвратите погрешно представяне или злоупотреба с данни
1. Уверете се, че вашата AI инфраструктура поддържа филтриране на съдържание

A:1, Въпреки че и трите препоръки са отлични, осигуряването на правилни привилегии за достъп до данни за потребителите ще бъде от голяма полза за предотвратяване на манипулация и погрешно представяне на данните, използвани от LLMs.

## 🚀 Предизвикателство

Прочетете повече за това как можете [да управлявате и защитавате чувствителна информация](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) в ерата на AI.

## Отлична работа, продължете обучението си

След като завършите този урок, разгледайте нашата [колекция за обучение по Генеративен AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), за да продължите да развивате знанията си за Генеративен AI!

Преминете към Урок 14, където ще разгледаме [Жизнения цикъл на приложенията за Генеративен AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или погрешни интерпретации, произтичащи от използването на този превод.