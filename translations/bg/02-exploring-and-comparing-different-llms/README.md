<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6b7629b8ee4d7d874a27213e903d86a7",
  "translation_date": "2025-10-17T22:18:03+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "bg"
}
-->
# Изследване и сравнение на различни LLMs

[![Изследване и сравнение на различни LLMs](../../../translated_images/02-lesson-banner.ef94c84979f97f60f07e27d905e708cbcbdf78707120553ccab27d91c947805b.bg.png)](https://youtu.be/KIRUeDKscfI?si=8BHX1zvwzQBn-PlK)

> _Кликнете върху изображението по-горе, за да гледате видеото към този урок_

В предишния урок разгледахме как Генеративният AI променя технологичния пейзаж, как работят големите езикови модели (LLMs) и как един бизнес - като нашия стартъп - може да ги приложи към своите случаи на употреба и да расте! В тази глава ще сравним и контрастираме различни типове големи езикови модели (LLMs), за да разберем техните предимства и недостатъци.

Следващата стъпка в пътя на нашия стартъп е да изследва текущия пейзаж на LLMs и да разбере кои са подходящи за нашия случай на употреба.

## Въведение

Този урок ще обхване:

- Различни типове LLMs в текущия пейзаж.
- Тестване, итерация и сравнение на различни модели за вашия случай на употреба в Azure.
- Как да внедрите LLM.

## Цели на обучението

След завършване на този урок ще можете:

- Да изберете правилния модел за вашия случай на употреба.
- Да разберете как да тествате, итерирате и подобрите производителността на вашия модел.
- Да знаете как бизнесите внедряват модели.

## Разбиране на различни типове LLMs

LLMs могат да бъдат категоризирани по различни начини въз основа на тяхната архитектура, тренировъчни данни и случаи на употреба. Разбирането на тези разлики ще помогне на нашия стартъп да избере правилния модел за даден сценарий и да разбере как да тества, итерира и подобри производителността.

Съществуват много различни типове LLM модели, изборът на модел зависи от това за какво искате да ги използвате, вашите данни, колко сте готови да платите и други фактори.

В зависимост от това дали искате да използвате моделите за текст, аудио, видео, генериране на изображения и т.н., може да изберете различен тип модел.

- **Разпознаване на аудио и реч**. За тази цел моделите от типа Whisper са отличен избор, тъй като са универсални и насочени към разпознаване на реч. Те са обучени на разнообразно аудио и могат да извършват многоезично разпознаване на реч. Научете повече за [моделите от типа Whisper тук](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Генериране на изображения**. За генериране на изображения DALL-E и Midjourney са две много известни опции. DALL-E се предлага от Azure OpenAI. [Прочетете повече за DALL-E тук](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) и също в глава 9 от тази учебна програма.

- **Генериране на текст**. Повечето модели са обучени за генериране на текст и имате голямо разнообразие от избори от GPT-3.5 до GPT-4. Те идват с различни разходи, като GPT-4 е най-скъпият. Струва си да разгледате [Azure OpenAI playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst), за да оцените кои модели най-добре отговарят на вашите нужди по отношение на възможности и разходи.

- **Мултимодалност**. Ако искате да обработвате множество типове данни във входа и изхода, може да разгледате модели като [gpt-4 turbo с визия или gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst) - най-новите версии на моделите на OpenAI - които са способни да комбинират обработка на естествен език с визуално разбиране, позволявайки взаимодействия чрез мултимодални интерфейси.

Изборът на модел означава, че получавате някои основни възможности, които обаче може да не са достатъчни. Често имате специфични за компанията данни, които трябва по някакъв начин да предоставите на LLM. Има няколко различни подхода за това, повече за тях в следващите секции.

### Основни модели срещу LLMs

Терминът Основен модел беше [въведен от изследователи от Станфорд](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) и се определя като AI модел, който следва някои критерии, като:

- **Те са обучени чрез неконтролирано обучение или самостоятелно контролирано обучение**, което означава, че са обучени на немаркирани мултимодални данни и не изискват човешка анотация или маркиране на данни за процеса на обучение.
- **Те са много големи модели**, базирани на много дълбоки невронни мрежи, обучени на милиарди параметри.
- **Обикновено са предназначени да служат като „основа“ за други модели**, което означава, че могат да се използват като отправна точка за изграждане на други модели, което може да се направи чрез фина настройка.

![Основни модели срещу LLMs](../../../translated_images/FoundationModel.e4859dbb7a825c94b284f17eae1c186aabc21d4d8644331f5b007d809cf8d0f2.bg.png)

Източник на изображението: [Essential Guide to Foundation Models and Large Language Models | by Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

За да изясним допълнително това разграничение, нека вземем ChatGPT като пример. За да се изгради първата версия на ChatGPT, моделът GPT-3.5 служи като основен модел. Това означава, че OpenAI използва някои специфични за чат данни, за да създаде настроена версия на GPT-3.5, която е специализирана в добро представяне в разговорни сценарии, като чатботове.

![Основен модел](../../../translated_images/Multimodal.2c389c6439e0fc51b0b7b226d95d7d900d372ae66902d71b8ce5ec4951b8efbe.bg.png)

Източник на изображението: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Отворен код срещу Собствени модели

Друг начин за категоризиране на LLMs е дали са с отворен код или собствени.

Моделите с отворен код са модели, които са достъпни за обществеността и могат да бъдат използвани от всеки. Те често се предоставят от компанията, която ги е създала, или от изследователската общност. Тези модели могат да бъдат инспектирани, модифицирани и персонализирани за различни случаи на употреба в LLMs. Въпреки това, те не винаги са оптимизирани за производствена употреба и може да не са толкова ефективни, колкото собствените модели. Освен това финансирането за модели с отворен код може да бъде ограничено и те може да не се поддържат дългосрочно или да не се актуализират с най-новите изследвания. Примери за популярни модели с отворен код включват [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) и [LLaMA](https://llama.meta.com).

Собствените модели са модели, които са собственост на компания и не са достъпни за обществеността. Тези модели често са оптимизирани за производствена употреба. Въпреки това, те не могат да бъдат инспектирани, модифицирани или персонализирани за различни случаи на употреба. Освен това, те не винаги са достъпни безплатно и може да изискват абонамент или плащане за използване. Потребителите нямат контрол върху данните, които се използват за обучение на модела, което означава, че трябва да се доверят на собственика на модела за ангажимент към поверителността на данните и отговорното използване на AI. Примери за популярни собствени модели включват [OpenAI models](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) или [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Вграждане срещу Генериране на изображения срещу Генериране на текст и код

LLMs могат също да бъдат категоризирани според изхода, който генерират.

Вгражданията са набор от модели, които могат да преобразуват текст в числова форма, наречена вграждане, което е числово представяне на входния текст. Вгражданията улесняват машините да разбират връзките между думите или изреченията и могат да бъдат използвани като входни данни от други модели, като модели за класификация или модели за клъстеризация, които имат по-добра производителност върху числови данни. Моделите за вграждане често се използват за трансферно обучение, където модел се изгражда за заместителна задача, за която има изобилие от данни, и след това теглата на модела (вгражданията) се използват повторно за други задачи. Пример за тази категория е [OpenAI embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Вграждане](../../../translated_images/Embedding.c3708fe988ccf76073d348483dbb7569f622211104f073e22e43106075c04800.bg.png)

Моделите за генериране на изображения са модели, които генерират изображения. Тези модели често се използват за редактиране на изображения, синтез на изображения и превод на изображения. Моделите за генериране на изображения често се обучават на големи набори от изображения, като [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), и могат да се използват за генериране на нови изображения или за редактиране на съществуващи изображения с техники като инпейнтинг, супер-резолюция и оцветяване. Примери включват [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) и [Stable Diffusion models](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Генериране на изображения](../../../translated_images/Image.349c080266a763fd255b840a921cd8fc526ed78dc58708fa569ff1873d302345.bg.png)

Моделите за генериране на текст и код са модели, които генерират текст или код. Тези модели често се използват за обобщение на текст, превод и отговаряне на въпроси. Моделите за генериране на текст често се обучават на големи набори от текст, като [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), и могат да се използват за генериране на нов текст или за отговаряне на въпроси. Моделите за генериране на код, като [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), често се обучават на големи набори от код, като GitHub, и могат да се използват за генериране на нов код или за поправяне на грешки в съществуващ код.

![Генериране на текст и код](../../../translated_images/Text.a8c0cf139e5cc2a0cd3edaba8d675103774e6ddcb3c9fc5a98bb17c9a450e31d.bg.png)

### Енкодер-Декодер срещу Само декодер

За да говорим за различните типове архитектури на LLMs, нека използваме аналогия.

Представете си, че вашият мениджър ви е дал задача да напишете тест за учениците. Имате двама колеги; единият отговаря за създаването на съдържание, а другият за прегледа му.

Създателят на съдържание е като модел Само декодер, той може да разгледа темата и да види какво вече сте написали, след което да напише курс въз основа на това. Те са много добри в писането на ангажиращо и информативно съдържание, но не са много добри в разбирането на темата и учебните цели. Някои примери за модели Само декодер са моделите от семейството GPT, като GPT-3.

Рецензентът е като модел Само енкодер, той разглежда написания курс и отговорите, забелязвайки връзката между тях и разбирайки контекста, но не е добър в генерирането на съдържание. Пример за модел Само енкодер би бил BERT.

Представете си, че можем да имаме и някой, който може да създава и преглежда теста, това е модел Енкодер-Декодер. Някои примери биха били BART и T5.

### Услуга срещу Модел

Сега нека поговорим за разликата между услуга и модел. Услугата е продукт, който се предлага от доставчик на облачни услуги и често е комбинация от модели, данни и други компоненти. Моделът е основният компонент на услугата и често е основен модел, като LLM.

Услугите често са оптимизирани за производствена употреба и често са по-лесни за използване от моделите, чрез графичен потребителски интерфейс. Въпреки това, услугите не винаги са достъпни безплатно и може да изискват абонамент или плащане за използване, в замяна на използването на оборудването и ресурсите на собственика на услугата, оптимизиране на разходите и лесно мащабиране. Пример за услуга е [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), която предлага план за плащане според използването, което означава, че потребителите се таксуват пропорционално на това колко използват услугата. Освен това, Azure OpenAI Service предлага сигурност на корпоративно ниво и рамка за отговорен AI върху възможностите на моделите.

Моделите са просто невронната мрежа, с параметрите, теглата и други. Позволявайки на компаниите да работят локално, обаче, ще трябва да закупят оборудване, да изградят структура за мащабиране и да закупят лиценз или да използват модел с отворен код. Модел като LLaMA е достъпен за използване, изисквайки изчислителна мощност за работа с модела.

## Как да тествате и итерирате с различни модели, за да разберете производителността в Azure

След като нашият екип е изследвал текущия пейзаж на LLMs и е идентифицирал някои добри кандидати за своите сценарии, следващата стъпка е да ги тества върху своите данни и работно натоварване. Това е итеративен процес, който се извършва чрез експерименти и измервания.
Повечето от моделите, които споменахме в предишните параграфи (модели на OpenAI, модели с отворен код като Llama2 и трансформъри на Hugging Face), са налични в [Model Catalog](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) в [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) е облачна платформа, създадена за разработчици, които искат да изграждат приложения за генеративен AI и да управляват целия жизнен цикъл на разработката - от експериментиране до оценка - като комбинират всички услуги на Azure AI в един централен хъб с удобен графичен интерфейс. Каталогът на модели в Azure AI Studio позволява на потребителя да:

- Намери интересуващия го основен модел в каталога - независимо дали е собствен или с отворен код, като филтрира по задача, лиценз или име. За по-добра търсимост моделите са организирани в колекции, като колекцията Azure OpenAI, колекцията Hugging Face и други.

![Каталог на модели](../../../translated_images/AzureAIStudioModelCatalog.3cf8a499aa8ba0314f2c73d4048b3225d324165f547525f5b7cfa5f6c9c68941.bg.png)

- Прегледа картата на модела, включително подробно описание на предназначението и обучителните данни, примерен код и резултати от оценка в библиотеката за вътрешни оценки.

![Карта на модела](../../../translated_images/ModelCard.598051692c6e400d681a713ba7717e8b6e5e65f08d12131556fcec0f1789459b.bg.png)

- Сравни бенчмаркове между модели и налични в индустрията набори от данни, за да оцени кой отговаря на бизнес сценария, чрез панела [Model Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst).

![Бенчмаркове на модели](../../../translated_images/ModelBenchmarks.254cb20fbd06c03a4ca53994585c5ea4300a88bcec8eff0450f2866ee2ac5ff3.bg.png)

- Направи фина настройка на модела върху персонализирани обучителни данни, за да подобри производителността на модела за конкретна задача, използвайки възможностите за експериментиране и проследяване на Azure AI Studio.

![Фина настройка на модела](../../../translated_images/FineTuning.aac48f07142e36fddc6571b1f43ea2e003325c9c6d8e3fc9d8834b771e308dbf.bg.png)

- Разположи оригиналния предварително обучен модел или версията с фина настройка за отдалечено прогнозиране в реално време - управлявано изчисление - или сървърлесс API крайна точка - [плащане според използването](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) - за да позволи на приложенията да го използват.

![Разполагане на модел](../../../translated_images/ModelDeploy.890da48cbd0bccdb4abfc9257f3d884831e5d41b723e7d1ceeac9d60c3c4f984.bg.png)

> [!NOTE]
> Не всички модели в каталога са налични за фина настройка и/или разполагане с плащане според използването. Проверете картата на модела за подробности относно възможностите и ограниченията на модела.

## Подобряване на резултатите от LLM

С нашия стартъп екип разгледахме различни видове LLM и облачна платформа (Azure Machine Learning), която ни позволява да сравняваме различни модели, да ги оценяваме върху тестови данни, да подобряваме производителността и да ги разполагаме на крайни точки за прогнозиране.

Но кога трябва да се обмисли фина настройка на модел вместо използване на предварително обучен? Има ли други подходи за подобряване на производителността на модела за конкретни задачи?

Съществуват няколко подхода, които бизнесът може да използва, за да постигне желаните резултати от LLM. Можете да изберете различни видове модели с различни степени на обучение при разполагане на LLM в производство, с различни нива на сложност, разходи и качество. Ето някои различни подходи:

- **Инженеринг на подсказки с контекст**. Идеята е да предоставите достатъчно контекст, когато задавате подсказка, за да гарантирате, че ще получите необходимите отговори.

- **Генериране с допълнително извличане на данни (RAG)**. Вашите данни може да съществуват в база данни или уеб крайна точка, например, за да се гарантира, че тези данни или част от тях са включени по време на задаване на подсказката, можете да извлечете съответните данни и да ги направите част от подсказката на потребителя.

- **Фино настроен модел**. Тук обучавате модела допълнително върху вашите собствени данни, което води до по-точен и отговарящ на вашите нужди модел, но може да бъде скъпо.

![Разполагане на LLM](../../../translated_images/Deploy.18b2d27412ec8c02871386cbe91097c7f2190a8c6e2be88f66392b411609a48c.bg.png)

Източник на изображението: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Инженеринг на подсказки с контекст

Предварително обучените LLM работят много добре върху общи задачи с естествен език, дори когато се извикват с кратка подсказка, като изречение за завършване или въпрос – така нареченото обучение „zero-shot“.

Въпреки това, колкото повече потребителят може да формулира своя въпрос с подробна заявка и примери – контекстът – толкова по-точен и близък до очакванията на потребителя ще бъде отговорът. В този случай говорим за „one-shot“ обучение, ако подсказката включва само един пример, и „few-shot“ обучение, ако включва множество примери. Инженерингът на подсказки с контекст е най-рентабилният подход за начало.

### Генериране с допълнително извличане на данни (RAG)

LLM имат ограничението, че могат да използват само данните, които са били използвани по време на тяхното обучение, за да генерират отговор. Това означава, че те не знаят нищо за събития, които са се случили след процеса на обучение, и не могат да получат достъп до непублична информация (като данни на компанията). Това може да бъде преодоляно чрез RAG, техника, която допълва подсказката с външни данни под формата на откъси от документи, като се вземат предвид ограниченията на дължината на подсказката. Това се поддържа от инструменти за векторни бази данни (като [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)), които извличат полезните откъси от предварително дефинирани източници на данни и ги добавят към контекста на подсказката.

Тази техника е много полезна, когато бизнесът няма достатъчно данни, време или ресурси за фина настройка на LLM, но все пак желае да подобри производителността за конкретна задача и да намали рисковете от измислици, т.е. изкривяване на реалността или вредно съдържание.

### Фино настроен модел

Фината настройка е процес, който използва трансферно обучение, за да „адаптира“ модела към специфична задача или за решаване на конкретен проблем. За разлика от few-shot обучението и RAG, тя води до създаване на нов модел с актуализирани тегла и отклонения. Изисква набор от обучителни примери, състоящи се от един вход (подсказката) и свързания с него изход (завършването). Това би бил предпочитаният подход, ако:

- **Използване на фино настроени модели**. Бизнесът би искал да използва фино настроени по-малко способни модели (като модели за вграждане), вместо високопроизводителни модели, което води до по-рентабилно и бързо решение.

- **Вземане предвид на латентността**. Латентността е важна за конкретен случай на употреба, така че не е възможно да се използват много дълги подсказки или броят на примерите, които трябва да бъдат научени от модела, не се вписва в ограничението за дължина на подсказката.

- **Актуализация**. Бизнесът разполага с много висококачествени данни и етикети на истината и ресурсите, необходими за поддържане на тези данни актуални с времето.

### Обучен модел

Обучението на LLM от нулата е без съмнение най-трудният и най-сложен подход за прилагане, изискващ огромни количества данни, квалифицирани ресурси и подходяща изчислителна мощност. Тази опция трябва да се обмисли само в сценарий, при който бизнесът има специфичен за дадена област случай на употреба и голямо количество данни, свързани с тази област.

## Проверка на знанията

Кой би бил добър подход за подобряване на резултатите от завършване на LLM?

1. Инженеринг на подсказки с контекст  
1. RAG  
1. Фино настроен модел  

A:3, ако разполагате с време, ресурси и висококачествени данни, фината настройка е по-добрият вариант за поддържане на актуалност. Въпреки това, ако искате да подобрите нещата и нямате достатъчно време, си струва първо да обмислите RAG.

## 🚀 Предизвикателство

Прочетете повече за това как можете да [използвате RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) за вашия бизнес.

## Отлична работа, продължете обучението си

След като завършите този урок, разгледайте нашата [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), за да продължите да развивате знанията си за генеративния AI!

Преминете към Урок 3, където ще разгледаме как да [изграждаме с генеративен AI отговорно](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да било недоразумения или погрешни интерпретации, произтичащи от използването на този превод.