<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-17T22:16:40+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "bg"
}
-->
# Използване на Генеративен ИИ отговорно

[![Използване на Генеративен ИИ отговорно](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.bg.png)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _Кликнете върху изображението по-горе, за да гледате видеото на този урок_

Лесно е да се впечатлите от ИИ и по-специално от генеративния ИИ, но е важно да обмислите как да го използвате отговорно. Трябва да вземете предвид неща като това как да гарантирате, че резултатите са справедливи, не са вредни и други. Тази глава има за цел да ви предостави този контекст, какво да обмислите и как да предприемете активни стъпки за подобряване на използването на ИИ.

## Въведение

Този урок ще обхване:

- Защо трябва да приоритизирате Отговорния ИИ при създаването на приложения с Генеративен ИИ.
- Основните принципи на Отговорния ИИ и как те се отнасят до Генеративния ИИ.
- Как да приложите тези принципи на Отговорния ИИ на практика чрез стратегии и инструменти.

## Цели на обучението

След завършване на този урок ще знаете:

- Значението на Отговорния ИИ при създаването на приложения с Генеративен ИИ.
- Кога да мислите и прилагате основните принципи на Отговорния ИИ при създаването на приложения с Генеративен ИИ.
- Какви инструменти и стратегии са на разположение, за да приложите концепцията за Отговорен ИИ на практика.

## Принципи на Отговорния ИИ

Вълнението около Генеративния ИИ никога не е било по-голямо. Това вълнение привлече много нови разработчици, внимание и финансиране в тази област. Въпреки че това е много положително за всеки, който иска да създава продукти и компании, използващи Генеративен ИИ, също така е важно да подходим отговорно.

През целия този курс се фокусираме върху изграждането на нашия стартъп и нашия образователен продукт с ИИ. Ще използваме принципите на Отговорния ИИ: Справедливост, Инклузивност, Надеждност/Безопасност, Сигурност и Поверителност, Прозрачност и Отговорност. С тези принципи ще изследваме как те се отнасят до използването на Генеративния ИИ в нашите продукти.

## Защо трябва да приоритизирате Отговорния ИИ

Когато създавате продукт, подходът, ориентиран към човека, като се има предвид най-добрият интерес на вашите потребители, води до най-добри резултати.

Уникалността на Генеративния ИИ е в неговата способност да създава полезни отговори, информация, насоки и съдържание за потребителите. Това може да се направи без много ръчни стъпки, което може да доведе до много впечатляващи резултати. Без подходящо планиране и стратегии, това обаче може да доведе и до някои вредни резултати за вашите потребители, вашия продукт и обществото като цяло.

Нека разгледаме някои (но не всички) от тези потенциално вредни резултати:

### Халюцинации

Халюцинациите са термин, използван за описание на случаи, когато LLM генерира съдържание, което е или напълно безсмислено, или нещо, което знаем, че е фактически грешно въз основа на други източници на информация.

Да вземем за пример, че създаваме функция за нашия стартъп, която позволява на учениците да задават исторически въпроси на модел. Ученик задава въпроса: `Кой беше единственият оцелял от Титаник?`

Моделът генерира отговор като този по-долу:

![Подканващ въпрос "Кой беше единственият оцелял от Титаник"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Източник: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Това е много уверен и подробен отговор. За съжаление, той е грешен. Дори с минимално количество изследвания, човек би открил, че е имало повече от един оцелял от катастрофата на Титаник. За ученик, който тепърва започва да изследва тази тема, този отговор може да бъде достатъчно убедителен, за да не бъде поставен под съмнение и да се приеме за истина. Последиците от това могат да доведат до ненадеждност на системата с ИИ и да навредят на репутацията на нашия стартъп.

С всяка итерация на даден LLM, наблюдаваме подобрения в производителността по отношение на минимизирането на халюцинациите. Дори с това подобрение, ние като създатели на приложения и потребители трябва да останем наясно с тези ограничения.

### Вредно съдържание

В предишния раздел разгледахме случаи, когато LLM генерира грешни или безсмислени отговори. Друг риск, който трябва да имаме предвид, е когато моделът отговаря с вредно съдържание.

Вредното съдържание може да бъде определено като:

- Предоставяне на инструкции или насърчаване на самонараняване или вреда на определени групи.
- Ненавистно или унизително съдържание.
- Насочване към планиране на всякакъв вид атаки или насилствени действия.
- Предоставяне на инструкции как да се намери незаконно съдържание или да се извършат незаконни действия.
- Показване на сексуално експлицитно съдържание.

За нашия стартъп искаме да се уверим, че разполагаме с правилните инструменти и стратегии, за да предотвратим този тип съдържание да бъде видяно от учениците.

### Липса на справедливост

Справедливостта се определя като „гарантиране, че системата с ИИ е свободна от пристрастия и дискриминация и че третира всички справедливо и равноправно.“ В света на Генеративния ИИ искаме да гарантираме, че изключващите възгледи за маргинализирани групи не се подсилват от резултатите на модела.

Тези видове резултати не само разрушават положителния опит на потребителите с нашите продукти, но също така причиняват допълнителна вреда на обществото. Като създатели на приложения, винаги трябва да имаме предвид широка и разнообразна потребителска база, когато изграждаме решения с Генеративен ИИ.

## Как да използваме Генеративен ИИ отговорно

Сега, когато идентифицирахме значението на Отговорния Генеративен ИИ, нека разгледаме 4 стъпки, които можем да предприемем, за да изградим нашите ИИ решения отговорно:

![Цикъл на смекчаване](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.bg.png)

### Измерване на потенциалните вреди

В софтуерното тестване тестваме очакваните действия на потребителя в приложението. По същия начин, тестването на разнообразен набор от подканващи въпроси, които потребителите най-вероятно ще използват, е добър начин за измерване на потенциалните вреди.

Тъй като нашият стартъп изгражда образователен продукт, би било добре да подготвим списък с подканващи въпроси, свързани с образованието. Това може да обхваща определени предмети, исторически факти и въпроси за живота на учениците.

### Смекчаване на потенциалните вреди

Сега е време да намерим начини, по които можем да предотвратим или ограничим потенциалните вреди, причинени от модела и неговите отговори. Можем да разгледаме това в 4 различни слоя:

![Слоеве на смекчаване](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.bg.png)

- **Модел**. Изборът на правилния модел за правилния случай на употреба. По-големите и по-сложни модели като GPT-4 могат да представляват по-голям риск от вредно съдържание, когато се прилагат към по-малки и по-специфични случаи на употреба. Използването на вашите тренировъчни данни за фина настройка също намалява риска от вредно съдържание.

- **Система за безопасност**. Системата за безопасност е набор от инструменти и конфигурации на платформата, която обслужва модела, и които помагат за смекчаване на вредите. Пример за това е системата за филтриране на съдържание в Azure OpenAI услугата. Системите също трябва да откриват атаки за заобикаляне на ограниченията и нежелани дейности като заявки от ботове.

- **Метаподканва**. Метаподканвите и основаването са начини, по които можем да насочим или ограничим модела въз основа на определени поведения и информация. Това може да включва използване на системни входове за определяне на определени ограничения на модела. Освен това, предоставянето на резултати, които са по-релевантни за обхвата или домейна на системата.

Може също така да се използват техники като Възстановително обогатено генериране (RAG), за да се гарантира, че моделът извлича информация само от избрани надеждни източници. Има урок по-късно в този курс за [създаване на приложения за търсене](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Потребителски интерфейс**. Последният слой е там, където потребителят взаимодейства директно с модела чрез интерфейса на нашето приложение. По този начин можем да проектираме UI/UX така, че да ограничим потребителя в типовете входни данни, които може да изпраща към модела, както и текста или изображенията, които се показват на потребителя. Когато внедряваме приложението с ИИ, също така трябва да бъдем прозрачни относно това, какво може и какво не може да прави нашето приложение с Генеративен ИИ.

Имаме цял урок, посветен на [Проектиране на UX за приложения с ИИ](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Оценка на модела**. Работата с LLM може да бъде предизвикателна, защото не винаги имаме контрол върху данните, върху които е обучен моделът. Независимо от това, винаги трябва да оценяваме производителността и резултатите на модела. Все още е важно да се измерва точността, сходството, обосноваността и релевантността на резултатите от модела. Това помага да се осигури прозрачност и доверие на заинтересованите страни и потребителите.

### Управление на отговорно решение с Генеративен ИИ

Изграждането на оперативна практика около вашите приложения с ИИ е последният етап. Това включва партньорство с други части на нашия стартъп, като правния и сигурностния отдел, за да се гарантира, че спазваме всички регулаторни политики. Преди пускането на пазара също така искаме да изградим планове за доставка, управление на инциденти и връщане назад, за да предотвратим всякаква вреда за нашите потребители.

## Инструменти

Въпреки че работата по разработването на решения с Отговорен ИИ може да изглежда много, тя е напълно оправдана. С развитието на областта на Генеративния ИИ ще се появяват все повече инструменти, които да помагат на разработчиците ефективно да интегрират отговорността в своите работни процеси. Например, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) може да помогне за откриване на вредно съдържание и изображения чрез API заявка.

## Проверка на знанията

Какви неща трябва да имате предвид, за да гарантирате отговорното използване на ИИ?

1. Че отговорът е правилен.  
2. Вредна употреба, че ИИ не се използва за престъпни цели.  
3. Гарантиране, че ИИ е свободен от пристрастия и дискриминация.  

О: 2 и 3 са правилни. Отговорният ИИ ви помага да обмислите как да смекчите вредните ефекти и пристрастията и още.

## 🚀 Предизвикателство

Прочетете повече за [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) и вижте какво можете да приложите за вашата употреба.

## Чудесна работа, продължете с обучението си

След като завършите този урок, разгледайте нашата [колекция за обучение по Генеративен ИИ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), за да продължите да развивате знанията си за Генеративния ИИ!

Преминете към Урок 4, където ще разгледаме [Основи на инженерството на подканващи въпроси](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да било недоразумения или погрешни интерпретации, произтичащи от използването на този превод.