<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-17T22:17:33+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "bg"
}
-->
# Извличане с допълнително генериране (RAG) и векторни бази данни

[![Извличане с допълнително генериране (RAG) и векторни бази данни](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.bg.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

В урока за приложения за търсене накратко разгледахме как да интегрирате свои данни в големи езикови модели (LLMs). В този урок ще се задълбочим в концепциите за свързване на вашите данни с приложението ви за LLM, механиката на процеса и методите за съхранение на данни, включително както вградени данни, така и текст.

> **Видео скоро ще бъде налично**

## Въведение

В този урок ще разгледаме следното:

- Въведение в RAG, какво представлява и защо се използва в изкуствения интелект (AI).

- Разбиране на векторните бази данни и създаване на такава за нашето приложение.

- Практически пример за интегриране на RAG в приложение.

## Цели на обучението

След завършване на този урок ще можете:

- Да обясните значението на RAG при извличането и обработката на данни.

- Да настроите приложение с RAG и да свържете данните си с LLM.

- Ефективно интегриране на RAG и векторни бази данни в приложения с LLM.

## Нашият сценарий: подобряване на LLM с наши собствени данни

За този урок искаме да добавим свои бележки към стартираща образователна платформа, която позволява на чатбота да предоставя повече информация за различни теми. Използвайки нашите бележки, учащите ще могат да учат по-ефективно и да разбират различните теми, което ще улесни подготовката за техните изпити. За да създадем нашия сценарий, ще използваме:

- `Azure OpenAI:` LLM, който ще използваме за създаване на нашия чатбот.

- `Урок за начинаещи по AI за невронни мрежи:` това ще бъдат данните, на които ще базираме нашия LLM.

- `Azure AI Search` и `Azure Cosmos DB:` векторна база данни за съхранение на нашите данни и създаване на индекс за търсене.

Потребителите ще могат да създават тренировъчни тестове от своите бележки, карти за преговори и да ги обобщават в кратки прегледи. За да започнем, нека разгледаме какво е RAG и как работи:

## Извличане с допълнително генериране (RAG)

Чатбот, захранван от LLM, обработва потребителски запитвания, за да генерира отговори. Той е проектиран да бъде интерактивен и да общува с потребителите по широк спектър от теми. Въпреки това, неговите отговори са ограничени до предоставения контекст и основните данни за обучение. Например, GPT-4 има ограничение на знанията до септември 2021 г., което означава, че му липсва информация за събития, случили се след този период. Освен това данните, използвани за обучение на LLM, изключват поверителна информация като лични бележки или ръководство за продукти на компанията.

### Как работят RAG (Извличане с допълнително генериране)

![диаграма, показваща как работят RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.bg.png)

Да предположим, че искате да внедрите чатбот, който създава тестове от вашите бележки, ще ви е необходима връзка с база знания. Тук идва на помощ RAG. RAG работи по следния начин:

- **База знания:** Преди извличането тези документи трябва да бъдат въведени и предварително обработени, обикновено чрез разделяне на големи документи на по-малки части, преобразуването им във вградени текстове и съхранението им в база данни.

- **Потребителско запитване:** Потребителят задава въпрос.

- **Извличане:** Когато потребителят зададе въпрос, моделът за вграждане извлича релевантна информация от нашата база знания, за да предостави повече контекст, който ще бъде включен в запитването.

- **Допълнително генериране:** LLM подобрява своя отговор въз основа на извлечените данни. Това позволява генерираният отговор да се основава не само на предварително обучени данни, но и на релевантна информация от добавения контекст. Извлечените данни се използват за допълване на отговорите на LLM. След това LLM връща отговор на въпроса на потребителя.

![диаграма, показваща архитектурата на RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.bg.png)

Архитектурата на RAG се реализира с помощта на трансформатори, състоящи се от две части: енкодер и декодер. Например, когато потребител зададе въпрос, входният текст се "кодира" във вектори, които улавят значението на думите, и векторите се "декодират" в нашия индекс на документи и генерират нов текст въз основа на потребителското запитване. LLM използва както модел енкодер-декодер, за да генерира изхода.

Два подхода при внедряването на RAG според предложената статия: [Извличане с допълнително генериране за задачи с интензивно използване на знания в NLP (естествено езиково обработване)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) са:

- **_RAG-Sequence_** използва извлечени документи, за да предскаже най-добрия възможен отговор на потребителското запитване.

- **RAG-Token** използва документи за генериране на следващия токен, след което ги извлича, за да отговори на потребителското запитване.

### Защо бихте използвали RAG? 

- **Богатство на информация:** гарантира, че текстовите отговори са актуални и съвременни. По този начин подобрява производителността при задачи, специфични за дадена област, като осигурява достъп до вътрешната база знания.

- Намалява измислиците, като използва **проверими данни** в базата знания, за да предостави контекст на потребителските запитвания.

- Това е **икономично**, тъй като е по-евтино в сравнение с фина настройка на LLM.

## Създаване на база знания

Нашето приложение се основава на нашите лични данни, т.е. урока за невронни мрежи от учебната програма AI For Beginners.

### Векторни бази данни

Векторната база данни, за разлика от традиционните бази данни, е специализирана база данни, предназначена за съхранение, управление и търсене на вградени вектори. Тя съхранява числови представяния на документи. Разбиването на данни на числови вграждания улеснява нашата AI система да разбира и обработва данните.

Съхраняваме нашите вграждания във векторни бази данни, тъй като LLM има ограничение на броя токени, които приема като вход. Тъй като не можете да предадете всички вграждания на LLM, ще трябва да ги разделите на части и когато потребител зададе въпрос, вгражданията, които най-много приличат на въпроса, ще бъдат върнати заедно със запитването. Разделянето на части също намалява разходите за броя токени, преминаващи през LLM.

Някои популярни векторни бази данни включват Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant и DeepLake. Можете да създадете модел на Azure Cosmos DB, използвайки Azure CLI със следната команда:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### От текст към вграждания

Преди да съхраним данните си, ще трябва да ги преобразуваме във векторни вграждания, преди да бъдат съхранени в базата данни. Ако работите с големи документи или дълги текстове, можете да ги разделите на части въз основа на запитванията, които очаквате. Разделянето може да се извърши на ниво изречение или на ниво параграф. Тъй като разделянето извлича значения от думите около тях, можете да добавите допълнителен контекст към частта, например, като добавите заглавието на документа или включите текст преди или след частта. Можете да разделите данните по следния начин:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

След като разделим текста, можем да го вградим, използвайки различни модели за вграждане. Някои модели, които можете да използвате, включват: word2vec, ada-002 от OpenAI, Azure Computer Vision и много други. Изборът на модел ще зависи от езиците, които използвате, типа съдържание, което се кодира (текст/изображения/аудио), размера на входа, който може да кодира, и дължината на изхода на вграждането.

Пример за вграден текст, използвайки модела `text-embedding-ada-002` на OpenAI:
![вграждане на думата котка](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.bg.png)

## Извличане и векторно търсене

Когато потребител зададе въпрос, извличащият го преобразува във вектор, използвайки енкодера за запитвания, след което търси в индекса на документи за релевантни вектори в документа, които са свързани с входа. След това преобразува както входния вектор, така и векторите на документите в текст и ги предава през LLM.

### Извличане

Извличането се случва, когато системата се опитва бързо да намери документи от индекса, които отговарят на критериите за търсене. Целта на извличащия е да получи документи, които ще бъдат използвани за предоставяне на контекст и свързване на LLM с вашите данни.

Има няколко начина за извършване на търсене в нашата база данни, като:

- **Търсене по ключови думи** - използва се за текстови търсения.

- **Семантично търсене** - използва семантичното значение на думите.

- **Векторно търсене** - преобразува документи от текст в векторни представяния, използвайки модели за вграждане. Извличането се извършва чрез запитване на документи, чиито векторни представяния са най-близки до потребителския въпрос.

- **Хибридно** - комбинация от търсене по ключови думи и векторно търсене.

Предизвикателство при извличането възниква, когато няма подобен отговор на запитването в базата данни. Системата тогава ще върне най-добрата информация, която може да намери. Въпреки това, можете да използвате тактики като задаване на максимално разстояние за релевантност или използване на хибридно търсене, което комбинира както ключови думи, така и векторно търсене. В този урок ще използваме хибридно търсене, комбинация от векторно и търсене по ключови думи. Ще съхраним данните си в dataframe с колони, съдържащи частите, както и вгражданията.

### Векторна сходност

Извличащият ще търси в базата знания за вграждания, които са близки едно до друго, най-близките съседи, тъй като те са текстове, които са подобни. В сценария, когато потребител зададе запитване, то първо се вгражда, след което се съпоставя с подобни вграждания. Общото измерване, което се използва за определяне на сходството между различни вектори, е косинусната сходност, която се основава на ъгъла между два вектора.

Можем да измерим сходството, използвайки други алтернативи, като Евклидово разстояние, което е правата линия между краищата на векторите, и точковото произведение, което измерва сумата от произведенията на съответните елементи на два вектора.

### Индекс за търсене

Когато извършваме извличане, ще трябва да изградим индекс за търсене за нашата база знания, преди да извършим търсене. Индексът ще съхранява нашите вграждания и може бързо да извлича най-подобните части, дори в голяма база данни. Можем да създадем индекс локално, използвайки:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Пренареждане

След като сте направили запитване към базата данни, може да се наложи да сортирате резултатите от най-релевантните. LLM за пренареждане използва машинно обучение, за да подобри релевантността на резултатите от търсенето, като ги подрежда от най-релевантните. Използвайки Azure AI Search, пренареждането се извършва автоматично чрез семантичен пренареждач. Пример за това как работи пренареждането, използвайки най-близките съседи:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Събиране на всичко заедно

Последната стъпка е добавянето на нашия LLM, за да можем да получим отговори, които са базирани на нашите данни. Можем да го реализираме по следния начин:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Оценка на нашето приложение

### Метрики за оценка

- Качество на предоставените отговори, като се гарантира, че звучат естествено, гладко и човешки.

- Базираност на данните: оценка дали отговорът идва от предоставените документи.

- Релевантност: оценка дали отговорът съответства и е свързан с зададения въпрос.

- Гладкост - дали отговорът има смисъл граматически.

## Приложения на RAG (Извличане с допълнително генериране) и векторни бази данни

Има много различни приложения, при които функциите на RAG могат да подобрят вашето приложение, като:

- Въпроси и отговори: свързване на данни на вашата компания с чат, който може да се използва от служителите за задаване на въпроси.

- Системи за препоръки: където можете да създадете система, която съпоставя най-подобните стойности, напр. филми, ресторанти и много други.

- Услуги за чатбот: можете да съхранявате историята на чата и да персонализирате разговора въз основа на данните на потребителя.

- Търсене на изображения въз основа на вградени вектори, полезно при разпознаване на изображения и откриване на аномалии.

## Резюме

Разгледахме основните аспекти на RAG, от добавянето на нашите данни към приложението, потребителското запитване и изхода. За да опростите създаването на RAG, можете да използвате рамки като Semantic Kernel, Langchain или Autogen.

## Задача

За да продължите обучението си за Извличане с допълнително генериране (RAG), можете да:

- Създадете потребителски интерфейс за приложението, използвайки избраната от вас рамка.

- Използвате рамка, като LangChain или Semantic Kernel, и да пресъздадете вашето приложение.

Поздравления за завършването на урока 👏.

## Обучението не спира тук, продължете пътешествието

След завършване на този урок, разгледайте нашата [колекция за обучение по Генеративен AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), за да продължите да развивате знанията си за Генеративен AI!

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да било недоразумения или погрешни интерпретации, произтичащи от използването на този превод.